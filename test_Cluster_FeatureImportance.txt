import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

# Load your dataframe
# Replace 'your_dataframe.csv' with the path to your actual dataframe
# df = pd.read_csv('your_dataframe.csv')

# Assuming df is your dataframe
# Example dataframe creation for illustration
np.random.seed(42)
df = pd.DataFrame({
    'X1': np.random.randn(100),
    'X2': np.random.randn(100),
    'X3': np.random.randn(100),
    'X4': np.random.randn(100),
    'Y': np.random.choice([0, 1], size=100)
})

# Split the data into features and target variable
X = df.drop('Y', axis=1)
y = df['Y']

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Calculate feature importances using a Random Forest classifier
rf = RandomForestClassifier(random_state=42)
rf.fit(X, y)
feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)

# Print the feature importances
print("Feature Importances:")
print(feature_importances)

# Determine the optimal number of clusters using the Elbow Method
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    sse.append(kmeans.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), sse)
plt.xlabel('Number of Clusters')
plt.ylabel('SSE')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.show()

# Assuming you choose k=3 based on the elbow method
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Add the cluster labels to the dataframe
df['Cluster'] = clusters

# Get the three most important features
most_important_features = feature_importances.index[:3]

# 3D scatter plot of the clusters on the most important features
fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(df[most_important_features[0]], df[most_important_features[1]], df[most_important_features[2]], c=df['Cluster'], cmap='viridis')
legend1 = ax.legend(*scatter.legend_elements(), title="Clusters")
ax.add_artist(legend1)
ax.set_xlabel(most_important_features[0])
ax.set_ylabel(most_important_features[1])
ax.set_zlabel(most_important_features[2])
plt.title('Clusters Visualization on Most Important Features')
plt.show()

############## Recursive Feature Elimination
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

# Load your dataframe
# Replace 'your_dataframe.csv' with the path to your actual dataframe
# df = pd.read_csv('your_dataframe.csv')

# Assuming df is your dataframe
# Example dataframe creation for illustration
np.random.seed(42)
df = pd.DataFrame({
    'X1': np.random.randn(100),
    'X2': np.random.randn(100),
    'X3': np.random.randn(100),
    'X4': np.random.randn(100),
    'Y': np.random.choice([0, 1], size=100)
})

# Split the data into features and target variable
X = df.drop('Y', axis=1)
y = df['Y']

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Use Recursive Feature Elimination with Cross-Validation to select features
rf = RandomForestClassifier(random_state=42)
rfecv = RFECV(estimator=rf, step=1, cv=5, scoring='accuracy')
rfecv.fit(X_scaled, y)

# Get the selected features
selected_features = X.columns[rfecv.support_]
print("Selected Features:")
print(selected_features)

# Determine the optimal number of clusters using the Elbow Method
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled[:, rfecv.support_])
    sse.append(kmeans.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), sse)
plt.xlabel('Number of Clusters')
plt.ylabel('SSE')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.show()

# Assuming you choose k=3 based on the elbow method
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_scaled[:, rfecv.support_])

# Add the cluster labels to the dataframe
df['Cluster'] = clusters

# Visualize the clusters on the three most important features
fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(df[selected_features[0]], df[selected_features[1]], df[selected_features[2]], c=df['Cluster'], cmap='viridis')
legend1 = ax.legend(*scatter.legend_elements(), title="Clusters")
ax.add_artist(legend1)
ax.set_xlabel(selected_features[0])
ax.set_ylabel(selected_features[1])
ax.set_zlabel(selected_features[2])
plt.title('Clusters Visualization on Selected Features')
plt.show()

##### Residualized Regression
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf

def run_regressions(data, Y, Xs, Zs):
    # Step 1: Regression of Y on Zs and saving residuals as Y_tilde
    formula_Y = f"{Y} ~ " + " + ".join(Zs)
    model_Y = smf.ols(formula=formula_Y, data=data).fit()
    data['Y_tilde'] = model_Y.resid

    # Step 2: Regression of each X on Zs and saving residuals as Xi_tilde
    for X in Xs:
        formula_X = f"{X} ~ " + " + ".join(Zs)
        model_X = smf.ols(formula=formula_X, data=data).fit()
        data[f'{X}_tilde'] = model_X.resid

    # Step 3: Regression of Y_tilde on all Xi_tilde's jointly
    tilde_columns = [f"{X}_tilde" for X in Xs]
    formula_tilde = "Y_tilde ~ " + " + ".join(tilde_columns)
    model_tilde = smf.ols(formula=formula_tilde, data=data).fit()

    return model_tilde

# Example usage:
# Assuming df is your dataframe with columns Y, X1, ..., Xp, Z1, ..., Zk
# Y = 'Y'
# Xs = ['X1', 'X2', 'Xp']
# Zs = ['Z1', 'Z2', 'Zk']
# model = run_regressions(df, Y, Xs, Zs)
# print(model.summary())


import pandas as pd
import statsmodels.api as sm

# Assuming you have a DataFrame called 'data' with columns Y, X1,...,Xp, Z1,...,Zk

# Step 1: Run regression of Y on Zs and save residuals as Y_tilde column
Zs = ['Z1', 'Z2', ..., 'Zk']
Xs = ['X1', 'X2', ..., 'Xp']

# Step 1: Run regression of Y on Zs and save residuals as Y_tilde column
def run_regression_y_on_zs(data, zs):
    X = data[zs]
    X = sm.add_constant(X)  # Adding a constant term to the model
    model = sm.OLS(data['Y'], X)
    results = model.fit()
    data['Y_tilde'] = results.resid
    return data

data = run_regression_y_on_zs(data, Zs)

# Step 2: Run regression of each X in Xs on Zs and save each residual as Xi_tilde column
def run_regression_x_on_zs(data, xs, zs):
    for x in xs:
        X = data[[x] + zs]
        X = sm.add_constant(X)  # Adding a constant term to the model
        model = sm.OLS(X[x], X.drop(columns=[x]))
        results = model.fit()
        data[x + '_tilde'] = results.resid
    return data

data = run_regression_x_on_zs(data, Xs, Zs)

# Step 3: Run regression of Y_tilde on all Xi_tilde's jointly
def run_regression_y_tilde_on_x_tildes(data, xs):
    X = data[[x + '_tilde' for x in xs]]
    X = sm.add_constant(X)  # Adding a constant term to the model
    model = sm.OLS(data['Y_tilde'], X)
    results = model.fit()
    print(results.summary())

run_regression_y_tilde_on_x_tildes(data, Xs)

##### Drop Variables from statsmodels
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf

def omit_coefficients_from_summary(model, omit_vars):
    summary = model.summary2().tables[1]
    for var in omit_vars:
        if var in summary.index:
            summary = summary.drop(index=var)
    return summary

# Example usage:
# Assuming df is your dataframe with columns Y, X1, ..., Xp, Z1, ..., Zk
# Y = 'Y'
# Xs = ['X1', 'X2', 'Xp']
# Zs = ['Z1', 'Z2', 'Zk']

# Create a formula and fit the model
formula = f"{Y} ~ " + " + ".join(Xs + Zs)
model = smf.ols(formula=formula, data=df).fit()

# Variables to omit from the summary
omit_vars = ['Z1', 'Z2']

# Get the modified summary
modified_summary = omit_coefficients_from_summary(model, omit_vars)
print(modified_summary)

import statsmodels.api as sm

# Assuming you have already run a linear regression and stored the results in the variable 'results'

# List of variable names you want to drop from the summary
variables_to_drop = ['var1', 'var2', ...]

# Dropping the variables from the results object
results_dropped = results.drop(variables_to_drop, axis=0)

# Printing the summary of the modified results
print(results_dropped.summary())

##### My Setup
#### Load Packages
import os
import numpy as np
import pandas as pd
import math
import statistics
import time
import re

import sklearn
import scipy.stats
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA

from sklearn.preprocessing import scale
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score

from statsmodels.api import OLS
import statsmodels.api as sm
import statsmodels.formula.api as smf
from linearmodels import PanelOLS

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

import seaborn as sns
import matplotlib.pyplot as plt
from binsreg import binsreg

# DateTme
from datetime import timedelta
from datetime import time

# Large Datasets
import dask.dataframe as dd

# Set Random Seed
import random
random.seed(1)

# Regression Output
from stargazer.stargazer import Stargazer
def stargazer_dropcols(latex_code, keywords_to_drop):
    lines = latex_code.split('\n')
    new_lines = []
    for line in lines:
        if not any(keyword in line for keyword in keywords_to_drop):
            new_lines.append(line)
    return '\n'.join(new_lines)

# Display All Outputs
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
pd.options.display.max_rows = 4000

# Plot Settings
plt.rcParams.update({'font.size': 24})


##### Regression Outputs
#reghdfe day`numdays'ret_cum_0 day0ret_this day0ret_abtailinv_this, nocons absorb(event) cluster(permno_id)
reg_original = smf.ols('day90ret_cum_0 ~ day0ret + day0ret_abtailinv_this', data=df_sample).fit(cov_type='cluster', cov_kwds={'groups': np.array(df_sample[['permno', 'date_actual']])})
reg_original.summary()

#df_sample.describe()

### Output to LaTeX
# Regression Results
stargazer = Stargazer([reg_original])
stargazer.covariate_order(['day0ret','day0ret_abtailinv_this'])
stargazer.rename_covariates({'day0ret':'Announcement-Day Return',
                             'day0ret_abtailinv_this':'Announcement-Day Return x Extremeness'})
stargazer.add_line("Specification",['Original'])

# XXX Replace Return with "Announcement-Day Return"
# XXX Replace Extremeness with "Extremeness"

keywords_to_drop = ['C(date',
                    'R^2','Residual Std. Error','F Statistic',
                    'Note:',
                    'begin{table}','end{table}',
                    'Dependent variable']

reg_output = stargazer_dropcols(stargazer.render_latex(), keywords_to_drop)

with open('output/ciq/RESTUD/reg_original.tex', 'w') as f:
    f.write(reg_output)

##### Feature Selection
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import SequentialFeatureSelector, RFE
from sklearn.metrics import mean_squared_error

# Example data
df = pd.DataFrame({
    'Y': np.random.rand(100),
    'X1': np.random.rand(100),
    'X2': np.random.rand(100),
    'X3': np.random.rand(100),
    'X4': np.random.rand(100),
    'X5': np.random.rand(100)
})

X = df.drop(columns=['Y'])
y = df['Y']

# Forward sequential feature selection
model = LinearRegression()
sfs_forward = SequentialFeatureSelector(model, n_features_to_select='auto', direction='forward', cv=5)
sfs_forward.fit(X, y)
selected_features_forward = X.columns[sfs_forward.get_support()]
print("Forward Sequential Feature Selection Selected Features:", selected_features_forward)

# Backward sequential feature selection
sfs_backward = SequentialFeatureSelector(model, n_features_to_select='auto', direction='backward', cv=5)
sfs_backward.fit(X, y)
selected_features_backward = X.columns[sfs_backward.get_support()]
print("Backward Sequential Feature Selection Selected Features:", selected_features_backward)

# Recursive Feature Elimination
rfe = RFE(model, n_features_to_select=3)  # Adjust n_features_to_select as needed
rfe.fit(X, y)
selected_features_rfe = X.columns[rfe.get_support()]
print("Recursive Feature Elimination Selected Features:", selected_features_rfe)

# Function definitions for step-wise selection
import statsmodels.api as sm
from itertools import combinations

def forward_stepwise_selection(data, response):
    initial_features = []
    remaining_features = list(data.columns)
    remaining_features.remove(response)
    selected_features = initial_features[:]
    best_mse = float('inf')
    best_model = None

    while remaining_features:
        mse_with_candidates = []
        for candidate in remaining_features:
            model = sm.OLS(data[response], sm.add_constant(data[selected_features + [candidate]])).fit()
            mse = mean_squared_error(data[response], model.predict(sm.add_constant(data[selected_features + [candidate]])))
            mse_with_candidates.append((mse, candidate))

        mse_with_candidates.sort()
        best_new_mse, best_candidate = mse_with_candidates[0]

        if best_new_mse < best_mse:
            selected_features.append(best_candidate)
            remaining_features.remove(best_candidate)
            best_mse = best_new_mse
            best_model = model
        else:
            break

    return best_model, selected_features

def backward_stepwise_selection(data, response):
    selected_features = list(data.columns)
    selected_features.remove(response)
    best_mse = float('inf')
    best_model = None

    while selected_features:
        mse_with_candidates = []
        for candidate in selected_features:
            remaining_features = list(selected_features)
            remaining_features.remove(candidate)
            model = sm.OLS(data[response], sm.add_constant(data[remaining_features])).fit()
            mse = mean_squared_error(data[response], model.predict(sm.add_constant(data[remaining_features])))
            mse_with_candidates.append((mse, candidate))

        mse_with_candidates.sort()
        best_new_mse, worst_candidate = mse_with_candidates[0]

        if best_new_mse < best_mse:
            selected_features.remove(worst_candidate)
            best_mse = best_new_mse
            best_model = model
        else:
            break

    return best_model, selected_features

def best_subset_selection(data, response):
    X = data.drop(columns=[response])
    y = data[response]
    best_mse = float('inf')
    best_model = None
    best_features = None

    for k in range(1, len(X.columns) + 1):
        for combo in combinations(X.columns, k):
            model = sm.OLS(y, sm.add_constant(data[list(combo)])).fit()
            mse = mean_squared_error(y, model.predict(sm.add_constant(data[list(combo)])))
            if mse < best_mse:
                best_mse = mse
                best_model = model
                best_features = combo

    return best_model, best_features

# Forward step-wise selection
model_forward, selected_features_forward = forward_stepwise_selection(df, 'Y')
print("Forward Step-wise Selected Features:", selected_features_forward)
print(model_forward.summary())

# Backward step-wise selection
model_backward, selected_features_backward = backward_stepwise_selection(df, 'Y')
print("Backward Step-wise Selected Features:", selected_features_backward)
print(model_backward.summary())

# Best subset selection
model_best_subset, selected_features_best_subset = best_subset_selection(df, 'Y')
print("Best Subset Selected Features:", selected_features_best_subset)
print(model_best_subset.summary())


##### Feature Selection (forward using statsmodels and adj. R^2)
import statsmodels.formula.api as smf


def forward_selected(data, response):
    """Linear model designed by forward selection.

    Parameters:
    -----------
    data : pandas DataFrame with all possible predictors and response

    response: string, name of response column in data

    Returns:
    --------
    model: an "optimal" fitted statsmodels linear model
           with an intercept
           selected by forward selection
           evaluated by adjusted R-squared
    """
    remaining = set(data.columns)
    remaining.remove(response)
    selected = []
    current_score, best_new_score = 0.0, 0.0
    while remaining and current_score == best_new_score:
        scores_with_candidates = []
        for candidate in remaining:
            formula = "{} ~ {} + 1".format(response,
                                           ' + '.join(selected + [candidate]))
            score = smf.ols(formula, data).fit().rsquared_adj
            scores_with_candidates.append((score, candidate))
        scores_with_candidates.sort()
        best_new_score, best_candidate = scores_with_candidates.pop()
        if current_score < best_new_score:
            remaining.remove(best_candidate)
            selected.append(best_candidate)
            current_score = best_new_score
    formula = "{} ~ {} + 1".format(response,
                                   ' + '.join(selected))
    model = smf.ols(formula, data).fit()
    return model

import pandas as pd


url = "http://data.princeton.edu/wws509/datasets/salary.dat"
data = pd.read_csv(url, sep='\\s+')

model = forward_selected(data, 'sl')

print model.model.formula
# sl ~ rk + yr + 1

print model.rsquared_adj
# 0.835190760538


